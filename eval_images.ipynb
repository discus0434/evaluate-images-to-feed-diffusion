{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Notebook for:\n",
    "\n",
    "### 1. Preprocess images to feed Stable Diffusion\n",
    "### 2. Evaluate how difficult the model grasp the features of your images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "from src.utils import ImageFolder, VAEHandler, denormalize, preprocess_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# image size, after preprocessing\n",
    "IMAGE_WIDTH = 512\n",
    "IMAGE_HEIGHT = 512\n",
    "\n",
    "# project root\n",
    "ROOT_DIR = Path(\"/evaluate-images-to-feed-diffusion\")\n",
    "\n",
    "# model directory\n",
    "MODEL_DIR = ROOT_DIR / \"models\"\n",
    "\n",
    "# vae model directory\n",
    "VAE_DIR = MODEL_DIR / \"waifu-diffusion-v1-4\"\n",
    "\n",
    "# focal model directory (to be used to crop images nicely)\n",
    "FOCAL_MODEL_DIR = MODEL_DIR / \"focal\"\n",
    "\n",
    "# raw image directory\n",
    "IMAGE_SOURCE_DIR = ROOT_DIR / \"images\"\n",
    "\n",
    "# processed image directory\n",
    "IMAGE_PREPROCESSED_DIR = ROOT_DIR / \"processed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Images\n",
    "\n",
    "Crop and convert images suitable for feeding model.\n",
    "\n",
    "If you do not leave `focal_model_dir=None`, focal model is automatically downloaded.\n",
    "\n",
    "Then, images are cropped in consideration of where the face / focal point is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_images(\n",
    "    IMAGE_SOURCE_DIR, \n",
    "    IMAGE_PREPROCESSED_DIR, \n",
    "    width=IMAGE_WIDTH, \n",
    "    height=IMAGE_HEIGHT, \n",
    "    focal_model_dir=FOCAL_MODEL_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_waifu_1_4 = VAEHandler(VAE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(0.5, 1)])\n",
    "dataset = ImageFolder(IMAGE_PREPROCESSED_DIR, transform).make_iterator(batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "The return value `res`  has: \n",
    "\n",
    "- Normalized tensor of original images\n",
    "- Latent `z`\n",
    "- Reconstructed tensors from `z`\n",
    "- Loss values of each images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = vae_waifu_1_4.get_loss_results(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "If there are some images whose loss value are quite high, model might not be able to learn the expressions of it well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = res.df.copy(deep=True)\n",
    "df.sort_index().plot(x=\"idx\", y=\"loss\", xlabel=\"image_idx\", ylabel=\"loss\", figsize=(7, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Worst and Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.plot_most_and_least_lossy_images(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in res.df.index:\n",
    "    plt.imshow(denormalize(np.array([res.rec[i]]))[0])\n",
    "    plt.title(f\"loss: {res.loss[i]}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7da40ff4c8facb3536fd59bbb3129f0e2a350e9761176d997558e12cea82dd53"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
